{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Dimensionnality Reduction\n",
    "---\n",
    "---\n",
    "Training millions of features is very slow weed could need to reduce the size of the features to speed up the training, in particular when it could be done without so much loss of  information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Curse of dimensionality\n",
    "---\n",
    "If the dataset has a lot of dimension, each instance will be unavoidable very sparse (each instance will be at great distance from each other). Consequently, the model will be less accurate than a model with lower dimension and the risk of overfitting is increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Main approaches for dimensionality reduction\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Projection\n",
    "When a training of dimension n set has an almost constant feature in the nth dimension for example, it's possible to project all the features in a dimension n-1. Example in 3D XYZ, if all values are almost constant regarding Z, the dataset could be projected on XY plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Manifold Learning\n",
    "It's the idea that higher dimension lie close to a manifold of lower dimension and that they could be equalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Dimensionality Reduction Algorithm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Principal Component Analysis (PCA)\n",
    "The main idea is to preserve as much as possible the variance when in dimension n when projecting on dimension n-1. Then it finds a second axis orthogonal to the fisrt axis with the second maximal variance etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal components could be find using Singular Value Decomposition (SVD). But be careful PCA assume that the dataset is centered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Calculating the Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0) # Dataset is centered\n",
    "U, s, Vt = p.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection down to d dimensions  \n",
    "To project X on the hyperplane of d dimension, X **centered** is multiplied by $W_{d}$ the matrix of the d first columns of V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2 # Example with d=2\n",
    "Wd = Vt.T[:, :d]\n",
    "Xd_proj = X_centered.dot(Wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Using sklearn\n",
    "No need to center X first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2) # 2 equal the dimension of the proj\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explained Variance Ratio\n",
    "explained_variance_ratio_ explicits the ratio of the variance that lies along the d first components of the PCA. (if the sum is closed to 100 the dimensionality waws the proper thing to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep 95% of the variance with dimensionality reduction you have 2 solutions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nÂ°1 solution\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ration_)\n",
    "d = np.argmax(cumsum=0.95)+1\n",
    "pca.n_components = d\n",
    "pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nÂ°2 solution\n",
    "pca = PCA(n_components=0.95) # n_components with a ratio between 0 and 1 indicating \n",
    "pca.fit_transform(X)         #the variance you want to preserve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Compression\n",
    "You can reconstruct the original dataset with a bit of lost information(among the percent of variance lost)  \n",
    "$X_{recovered} = X_{d-proj}W_{d}^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized PCA\n",
    "You can quickly find approximate PCA with stochastic approach, by setting the svd_solver to randomized,  \n",
    " - by default svd_solver=auto -> uses randomized if $dimension>500$ and d<0.8\\*dimension.  \n",
    " - set svd_solver to full if you want the compute full SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154, svd_solver=\"randomized\")\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental PCA\n",
    "On large dataset it could be complicated to compute PCA on all the dataset. Fortunately, Incremental PCA is used splitting the dataset in mini batches for computing PCA. It is useful for :\n",
    "- Large dataset\n",
    "- Online dataset (with on the fly data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "\n",
    "# PCA is fitted for each mini batches\n",
    "for X_batch in np.array.split(X_train, n_batches):\n",
    "    inc_pca.fit(X_batch)\n",
    "\n",
    "# Then X is reduced with X train\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you manipulate large array stored on disk file it's still possible to use IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m,n))\n",
    "\n",
    "batch_size = m//n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('ml_env': venv)",
   "language": "python",
   "name": "python36964bitmlenvvenv3cb243bcd6074c26927ee1e8d622f090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
