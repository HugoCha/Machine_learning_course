{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Dimensionnality Reduction\n",
    "---\n",
    "---\n",
    "Training millions of features is very slow weed could need to reduce the size of the features to speed up the training, in particular when it could be done without so much loss of  information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Curse of dimensionality\n",
    "---\n",
    "If the dataset has a lot of dimension, each instance will be unavoidable very sparse (each instance will be at great distance from each other). Consequently, the model will be less accurate than a model with lower dimension and the risk of overfitting is increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Main approaches for dimensionality reduction\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Projection\n",
    "When a training of dimension n set has an almost constant feature in the nth dimension for example, it's possible to project all the features in a dimension n-1. Example in 3D XYZ, if all values are almost constant regarding Z, the dataset could be projected on XY plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Manifold Learning\n",
    "It's the idea that higher dimension lie close to a manifold of lower dimension and that they could be equalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Dimensionality Reduction Algorithm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Principal Component Analysis (PCA)\n",
    "The main idea is to preserve as much as possible the variance when in dimension n when projecting on dimension n-1. Then it finds a second axis orthogonal to the fisrt axis with the second maximal variance etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal components could be find using Singular Value Decomposition (SVD). But be careful PCA assume that the dataset is centered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Calculating the Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0) # Dataset is centered\n",
    "U, s, Vt = p.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection down to d dimensions  \n",
    "To project X on the hyperplane of d dimension, X **centered** is multiplied by $W_{d}$ the matrix of the d first columns of V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2 # Example with d=2\n",
    "Wd = Vt.T[:, :d]\n",
    "Xd_proj = X_centered.dot(Wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Using sklearn\n",
    "No need to center X first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2) # 2 equal the dimension of the proj\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explained Variance Ratio\n",
    "explained_variance_ratio_ explicits the ratio of the variance that lies along the d first components of the PCA. (if the sum is closed to 100 the dimensionality waws the proper thing to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep 95% of the variance with dimensionality reduction you have 2 solutions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n째1 solution\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ration_)\n",
    "d = np.argmax(cumsum=0.95)+1\n",
    "pca.n_components = d\n",
    "pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n째2 solution\n",
    "pca = PCA(n_components=0.95) # n_components with a ratio between 0 and 1 indicating \n",
    "pca.fit_transform(X)         #the variance you want to preserve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Compression\n",
    "You can reconstruct the original dataset with a bit of lost information(among the percent of variance lost)  \n",
    "$X_{recovered} = X_{d-proj}W_{d}^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized PCA\n",
    "You can quickly find approximate PCA with stochastic approach, by setting the svd_solver to randomized,  \n",
    " - by default svd_solver=auto -> uses randomized if $dimension>500$ and d<0.8\\*dimension.  \n",
    " - set svd_solver to full if you want the compute full SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154, svd_solver=\"randomized\")\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental PCA\n",
    "On large dataset it could be complicated to compute PCA on all the dataset. Fortunately, Incremental PCA is used splitting the dataset in mini batches for computing PCA. It is useful for :\n",
    "- Large dataset\n",
    "- Online dataset (with on the fly data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "\n",
    "# PCA is fitted for each mini batches\n",
    "for X_batch in np.array.split(X_train, n_batches):\n",
    "    inc_pca.fit(X_batch)\n",
    "\n",
    "# Then X is reduced with X train\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you manipulate large array stored on disk file it's still possible to use IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m,n))\n",
    "\n",
    "batch_size = m//n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA\n",
    "Kernel PCA is an unsupervised learning technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding best Kernel hyperparameters n째1\n",
    "For Finding the best KPCA we could proceed as follow to automate the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    \"kpca_gamma\" : np.linspace(0.03, 0.05, 10),\n",
    "    \"kpca_kernel\" : [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding best Kernel hyperparameters n째2\n",
    "To find the best kPCA we could also use an unsupervised technique and keeping the kPCA with the lowest reconstruction error. However the reconstruction is not the same as linear PCA with inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# By setting fit_inverse_transform we specify we want to reconstruct the reduction\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "# The goal is to minimize mean_squared_error\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Locally Linear Embedding (LLE)\n",
    "Nonlinear Dimensionnality reduction: looking how neighboors of each instance are linearly related and then tried to reduce the dimension while preserving the linear relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Random projections\n",
    "Random linear projections preserves well the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Multidimensional Scaling\n",
    "Reduces the dimensionality while trying to preserve the distances between instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Isomap\n",
    "Creates a graph by connecting each instance to its nearest neighbors, thn reduce dimensionality while trying preserving geodesic distances between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "Reduces dimensionality while triyng to keep similar instances close and dissimilar instances apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g) Linear Discriminant Analysis (LDA) \n",
    "Classification Algo, during training learns the most discriminative axes between the classes, and these axezs can be used to form hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('ml_env': venv)",
   "language": "python",
   "name": "python36964bitmlenvvenv3cb243bcd6074c26927ee1e8d622f090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
